# 🍵 빠르게 살펴보는 카프카

### 🐸 2장에서 다루는 내용

- 카프카 아키텍처 조사
- 프로듀서로 메시지 보내기
- 컨슈머로 메시지 읽기
- 카프카 설치 및 실행

카프카 스트림즈는 카프카에 대해 설명하지 않고 탐구하는 것은 불가능하다. 결국 카프카 스트림즈는 카프카에서 실행하는 라이브러리이다.

## 👟데이터 문제

오늘날 조직은 데이터에 파묻혀 있다. 인터넷 기업, 금융 및 고객에게 더 나은 서비스를 제공하고 좀 더 효율적인 비즈니스 수행 방법을 찾기위해 데이터를 

사용하는데 어느 때보다 나은 위치에 있다. gs25데이터 관리 솔루션에 있을법한 요구사항을 분석해보자

- 중앙 저장소로 데이터를 신속하게 전송할 방법이 필요하다
- 서버가 자주 고장나서 데이터를 복제할 수 있는 기능도 필요하므로 피할 수 없는 오류로 인해 가동 중지 시간과 데이터 손실이 발생하지않아야한다
- 각기 다른 애플리케이션을 추적하지 않고도 많은 수의 데이터 컨슈머로 확장할 수 있는 잠재력이 필요하다. 조직의 모든 사용자가 데이터를 사용할 수 있도록
- 해야하지만, 누가 데이터를 보았는지 또는 보지 않았는지 추적할 필요는 없다.

## 🏓 카프카를 사용해 데이터 다루기 

### gs25의 오리지널 데이터 플랫폼

원래 gs25는 서로 다른 애플리케이션이 있는 시스템에 유입되는 소매 판매 데이터를 가진 소규모회사였다. 처음에는 잘 동작했지만 시간이 지나면서 새로운

접근법이 필요하다는 것이 분명해졌다. 한부서의 판매 데이터는 해당 부서만의 관심사항이 아니다. 회사의 여러 부서에 관심이 있으며 각 부서마다 중요한

내용과 원하는 데이터 구조가 다르다. 아래의 그림은 gs25의 기존 데이터 플랫폼을 보여준다

![image](https://user-images.githubusercontent.com/40031858/112090619-39453b00-8bd7-11eb-8dec-567bb07734eb.png)



시간이 지나면서 다른회사를 인수하고 기존 매장에서 상품을 늘려가며 지속적으로 성장했다. 하지만 추가할 때마다 애플리케이션 간의 연결이 복잡해졌다. 

![image](https://user-images.githubusercontent.com/40031858/112090996-e1f39a80-8bd7-11eb-82a0-c9898e2a20f6.png)



### 카프카 판매 거래 데이터 허브

Gs25 문제의 해결책은 모든 거래 데이터를 수용하는 하나의 유입 프로세스를 만드는 것이다. 이 트랜잭션 데이터 허브는 상태가 없어야 하며 거래 데이터를 받아들이고 모든

컨슈머 애플리케이션이 필요한 데이터를 가져갈 수 있는 방법으로 저장해야한다. 

`카프카` 는 내고장성을 가진 견고한 `발행/구독` 시스템이다. 하나의 카프카 노드를 `브로커`라고 부르고 여러개의 카프카 브로커 서버가 `클러스터` 를구성한다.

카프카는 `프로듀서` 가 작성한 메시지를 토픽에 저장한다. `컨슈머` 는 토픽을 구독하며, 구독한 토픽에 메시지가 있는지 확인하기 위해 카프카에 접속한다.

![image](https://user-images.githubusercontent.com/40031858/112091520-e8cedd00-8bd8-11eb-8715-d52e16aadfec.png)

## 🍰 카프카 아키텍처

### 🌟 카프카는 `메시지 브로커`다

위에서 카프카는 `발행/구독` 시스템이라고 언급했지만 `메시지 브로커` 역할을 한다고하는 편이 더 정확할 것이다. `브로커`는  상호 간 유익한 교환이나 거래를 위해 각자 반드시 알 필요 

없는 부분을 묶는 중개자이다 .

![image-20210323132042156](/Users/kimjunseong/Library/Application Support/typora-user-images/image-20210323132042156.png)

위의 그림은 gs25 데이터 인프라의 발전을 보여준다. 프로듀서와 컨슈머가 추가되어 개별 부문이 카프카로 통신하는 방법을 보여준다. 그들 서로는 직접

통신하지 않는다. 카프카는 토픽에 메시지를 저장하고 토픽에서 메시지를 검색한다. 메시지의 프로듀서와 컨슈머 간의 직접적인 연결은 없다. 또한 카프카는

프로듀서나 컨슈머에 관한 어떤 상태도 유지하지 않는다. 메시지 교환소로만 작동한다. 카프카 토픽의 내부 기술은 카프카가 들어오는 레코드를 기록한 파일인

로그다. 토픽에 들어오는 메시지의 부하를 관리하기 위해 카프카는 `파티션` 을 사용한다. `파티션` 의 한가지 용도는 각기 다른서버에 위치한 데이터를 동일한 서버에

함께 모으는 것이다.

### 🌟 `카프카는 로그다 `

카프카의 기본 메커니즘은 [`로그`](http://mng.b2/eE3w) 다. 애플리케이션에 성능 문제나 오류가 있다면 먼저 확인해야 할 것이 애플리케이션 로그지만 그건 다른 종류이ㅡ 로그다.

카프카의 맥락에서 로그는 `"추가 가능한 시간순으로 완전히 정렬된 레코드 시퀀스"` 다. 아래의 그림은 로그가 어떻게 생겼는지 보여준다.

![image](https://user-images.githubusercontent.com/40031858/112092846-b7a3dc00-8bdb-11eb-98b0-76a35c041c8d.png)

#### `로그는 들어오는 레코드가 추가되는 파일이다. 새로 도착한 각 레코드는 마지막 레코드 뒤에 위치한다. 이 과정은 파일의 레코드를 시간순으로 정렬한다`

`로그` 는 강력한 의미를 가진 단순한 데이터 추상화다. 시간과 관련해 순서대로 레코드를 보유하고 있다면 충돌을 해결하거나 다른 머신에 어떤 업데이트를 적용할지

결정하는 것이 단순해진다. 최신 레코드가 우선한다. 카프카의 `토픽` 은 토픽 이름으로 분리된 로그다. 토픽은 라벨이 붙은 로그라고 할 수 있다. 

로그가 머신 클러스터 간에 복제된 후 하나의 머신이 중지됨녀 해당 서버를 복구하는 것은 쉽다. 단지 로그파일을 재생하면 된다. 실패로부터 복구하는 기능은

정확하게 분산 커밋 로그의 역할이다. 



### 🧇 카프카에서 로그가 동작하는 방식

카프카를 설치할 때 설정 중 하나는 log.dir이며, 카프카가 로그 데이터를 저장하는 위치를 지정한다. 각 토픽은 지정된 로그 디렉토리 아래의 하위 디렉토리에

매핑된다. '파티션 이름... 파티션번호'의 형식으로 토픽 파티션 수만큼 하위 디렉토리가 있을것이다. 

각 디렉토리 안에는 들어오는 메시지가 추가되는 로그 파일이 있다. 로그 파일이 특정 크기에 도달하거나 메시지 타임스탬프 간에 구성된 시간 차이에 도달하면

로그 파일을 교체하고, 카프카는 들어오는 메시지를 새 로그에 추가한다.

![image](https://user-images.githubusercontent.com/40031858/112094140-0ce0ed00-8bde-11eb-87c1-4212a8505000.png)

#### `logs 디렉토리는 메시지의 기본 저장소다./logs 아래의 각 디렉토리는 토픽의 파티션을 나타낸다`

#### `디렉토리 내의 파일 이름은 토픽의 이름으로 시작하고 밑줄 표시와 파티션 번호가 붙는다`

로그와 토픽이 고도로 연결된 개념임을 알 수 있다. 토픽은 로그라고 하거나 로그를 나타낸다고 말할 수 있다. 토픽 이름은 프로듀서를 통해 카프카에 보내진 메시지가

저장될 로그를 잘 처리할 수 있게 해준다.

### 🏭 `카프카와 파티션`

파티션은 카프카 디자인에서 중요한 부분이다. 성능에 필수적이며, 같은 키를 가진 데이터가 동일한 컨슈머에게 전송되도록 보장한다. 

아래의 그림은 파티션의 작동방식을 보여준다.

![image](https://user-images.githubusercontent.com/40031858/112097603-a8289100-8be3-11eb-947b-aa396b2b4b70.png)



#### `카프카는 파티션을 사용해 높은 처리량을 달성하고 클러스터의 여러 머신에 각 토픽의 메시지를 분산한다`

토픽을 파티션으로 분할하면 기본적으로 병렬 스트림에서 토픽에 전달되는 데이터가 분할되는데, 이는 카프카가 엄청난 처리량을 달성하는 비결이다. 토픽이

분산로그라고 설명했었는데 각 파티션은 유사하게 로그 자체이며 동일한 규칙을 따른다. 카프카는 들어오는 각 메시지를 로그 끝에 추가하며 모든 메시지는

엄격하게 시간 순서가 지정된다. 각  메시지에는 할당된 오프셋 번호가 있다.` 파티션 간의 메시지 순서는 보장되지 않지만 각 파티션 내의 메시지 순서는 보장된다`

파티션으로 분할하는 것은 처리량 증가 외에도 다른 효과가 있다. 토픽의 메시지를 여러 머신에 분산해 특정 토픽의 용량이 한 서버의 사용 가능한 디스크 공간에

제한되지 않는다. 이제 파티션이 하는 또다른 중요한 역할도 살펴보자. 같은 키를 가진 메시지를 함께 처리하도록 보장한다

### 💕 키에 의한 그룹 데이터 분할

카프카는 `키/값` 쌍으로 데이터를 다룬다. 키가 널 이면 카프카 프로듀서는 `라운드로빈 방식`으로 선택된 파티션에 레코드를 쓴다.

아래의 그림은 널이 아닌 키로 파티션 할당이 어떻게 작동하는지 보여준다

![image](https://user-images.githubusercontent.com/40031858/112098407-13269780-8be5-11eb-8e59-36ced524de55.png)



#### `foo는 파티션 0으로 전송되고, bar는 파티션1에 전송된다. 키의 바이트를 해싱하고 파티션 수를 모듈로 계산해 파티션을 얻는다`

키가 널이 아닌 경우 카프카는 다음공식을 사용해 키 /값 쌍을 보낼 파티션을 결정한다

​	HashCode.(key)%파티션 수

`결정론적 접근법` 을 사용해 파티션을 선택하면 동일한 키를 가진 레코드가 `항상`  동일한 파티션에 순서대로 전송된다. 기본 파티셔너는 이방법을 사용한다

파티션을 선택하는데 다른 전략이 필요한 경우 사용자 정의 파티셔너를 제공할 수 있다.

### 사용자 정의 파티셔너 작성하기

왜 사용자 정의 파티셔너를 작성하는가? 몇가지 이유가 있지만 복합키를 사용하는 간단한 예시를 보자

카프카에 구매 데이터가 유입되고 있고 키에 고객ID와 거래 날짜라는 두 가지 값이 포함되어 있다고 가정하자. 그러나 고객ID로 값을 그룹

지어야 하므로 고객ID와 구매 날짜의 해시를 사용하면 작동하지 않을 것이다. 이 경우 사용자 정의 파티셔너를 작성해서 복합키의 어떤 부분이 어떤

파티션을 사용하는지를 결정해야한다.

#### `PurchaseKey` 복합키

```java
public class PurchaseKey{
  private String customerId;
  private Date transcationDate;
  
  public PurchaseKey(String customerId,Date transactionDate){
    this.customerId=customerId;
    this.transcationDate=transactionDate;
  }
  public String getCustomerId(){
    return customerId;
  }
  public Date getTransactionDate(){
    return transactionDate;
  }
}
```



파티셔닝과 관련해 특정 고객에 대한 모든 트랜잭션이 동일한 파티션으로 이동해야 하지만, 키 전체를 사용하면 이 작업을 수행할 수 없다. 며칠 동안 구매가 발생하면 날짜가 포함

되어 같은 고객이라도 키값이 달라지기 때문에 거래를 임의의 파티션에 배치하게 된다. 같은 고객 ID를 가진 거래는 같은 파티션에 보내도록 해야한다. 이렇게 하는 유일한 방법은

파티션을 결정할 때 고객 ID만 사용하는 것이다 . 다음예제는 사용자 정의 파티셔너가 필요한 작업을 수행한다. 키에서 고객 ID를 추출하고 사용할 파티션을 결정한다

```java
public class PurchaseKeyPartitioner extends DefaultPartitioner{
  @Override
  public int partition(String topic, Object key, byte[] keyBytes, Object value,
                      byte[] valueBytes, Cluster cluster){
    Object newKey=null;
    if(key != null){
      PurchaseKey purchaseKey=(PurchaseKey) key;
      newKey=purchaseKey.getCustomerId();
      keyBytes=((String) newKey).getBytes();
    }
    return super.partition(topic,newKey,keyBytes,value,valueBytes,cluster);
  }
}
```



이 사용자 정의 파티셔너는 `DefaultPartitioner` 를 확장한다. Partitioner인터페이스를 직접 구현할 수 도 있지만 이경우에는 사용하는 DefaultPartitioner에 기존로직이있다.

사용자 정의 파티셔너를 만들 때 키만 사용 가능한 것은 아님을 기억하자. 값만을 사용하거나 키와 값을 조합해 사용하는 것도 가능하다. 

### 👽 `사용자 정의 파티셔너 지정하기`

사용자 정의 파티셔너를 작성했으므로 기본 파티셔너 대신 사용하도록 카프카에게 알려야한다. 카프카 프로듀서를 설정할 때 파티셔너를 지정한다

​	partitioner.class=saechimdaeki.partitioner.PurchaseKeyPartitioner

프로듀서 인스턴스 별로 파티셔너를 설정할 수 있기때문에 프로듀서마다 각기 다른 파티셔너를 자유롭게 사용할 수 있다. 카프카 프로듀서 사용을 다룰 때 프로듀서 설정을 자세히 살펴보자

```markdown
`주의`:   사용할 키를 선택할 때와 파티셔닝할 키/값 쌍의 부분을 선택할 때 주의해야한다. 선택한 키가 모든 데이터에 대해 공평하게 분배하는지 확인하자. 그렇지 않으면
대부분의 데이터가 일부 파티션에만 위치하기 때문에 데이터가 특정 파티션에 몰리는 문제가 발생할 것이다. 
```



### `정확한 파티션 수 정하기`

핵심 고려사항 중 하나는 주어진 토픽에 들어오는 데이터 양이다. 데이터가 많을수록 처리량을 높이기 위해서는 더 많은 파티션이 필요하다. 그러나 이는 `트레이드 오프`가 있다.

파티션 수를 늘리면 TCP 연결수와 열린 파일 핸들 수 가 증가한다. 또한 컨슈머가 유입 레코드를 처리하는데 걸리는 시간도 처리량을 결정한다. 컨슈머가 대량의 데이털르 

처리하는 경우에는 파티션을 추가하면 도움이 되지만, 궁극적으로 처리 속도가 느려지만 성능이 저하된다.

### `분산 로그`

일반적으로 카프카 프로덕션 클러스터 환경에는 여러 대의 머신이 포함되어 있다. 하나의 노드를 고려할 때 개념을 이해하기가 더 쉽기 때문에 의도적으로 단일 노드를 중심적으로 

논의 했었으나 현실적으로는 머신 클러스터에서 카프카를 운영할 것이다. 토픽이 분할되면 카프카는 하나의 머신에 모든 파티션을 할당하지는 않는다. 카프카는 클러스터를 구성하는 

여러 머신에 파티션을 분산한다. 카프카는 레코드를 로그에 추가하므로 카프카는 이러한 레코드를 파티션별로 여러 머신에 분산한다. 

![image](https://user-images.githubusercontent.com/40031858/112101091-731f3d00-8be9-11eb-8b45-29ccf4c78033.png)



#### `프로듀서는 토픽의 파티션에 메시지를 작성한다. 메시지에 키가 없으면 프로듀서는 라운드 로빈방식으로 파티션을 선택한다.`

#### `그렇지 않으면 키의 해시를 파티션 수로 모듈러 연산한다`

프로듀서는 첫 번째 메시지를 카프카 브로커 1의 파티션 0에 보내고, 두번째 메시지는 카프카 브로커 1의 파티션 1에, 세번째 메시지는 카프카 브로커 2의 파티션2에 보낸다.

프로듀서가 여섯 번째 메시지를 보냄녀 카프카 브로커3의 파티션5로 가고, 다음메시지는 다시 카프카 브로커 1의 파티션0에 보낸다. 메시지 발행은 이런 방식으로 계속 진행되

며 카프카 클러스터의 모든 노드에 메시지 트래픽을 분산한다. 서버는 내려갈 수 있기 때문에 데이터를 원격에 저장한다는 사실이 위험스럽게 보일지모르지만 카프카는 `데이터 중복`

을 제공한다. 카프카에서 하나의 브로커에 저장하면 클러스터의 하나 혹은 그 이상의 머신에 데이터를 복제한다

### 주키퍼: 리더, 팔로워 , 복제 

이제 카프카가 머신 실패에 대비해 어떻게 데이터 가용성을 제공하는지 알아보자.

카프카는 리더와 팔로워 브로커라는 개념이 있다. 카프카에서 각 토픽 파티션별로 한 브로커가 다른 브로커의 `리더` 로 선택된다. 리더의 주요 임무 중 하나는 팔로워 브로커에 토픽 파

티션의 `복제` 를 할당하는 것이다. 카프카가 클러스터 전반에 걸쳐 토픽에 대한 파티션을 할당하는 것처럼 카프카는 파티션도 여러 머신에 복제한다.

리더, 팔로워 및 복제 작업방식의 세부사항을 보기전에 사용하는 기술부터 알아보자

### 아파치 주키퍼

`아파치 주키퍼` 는 카프카의 아키텍처에 필수적이며 주키퍼를 사용해 카프카가 리더 브로커를 확보하고 토픽 복제를 추적할 수 있게 한다.

```markdown
`주키퍼`는 구성정보를 유지 관리하고 이름을 지정하며 분산 동기화를 제공하고 그룹 서비스를 제공하는 중앙 집중식 서비스다. 이런 유형의 서비스는 분산된 애플리케이션에
의해 어떤 형태로든 사용된다.
```



카프카가 분산 애플리케이션이라는 점을 고려해 카프카 아키텍처에 주키퍼가 어떻게 관여하는지 알아야한다. 이를 위해 카프카 서버를 2대이상 설치한 경우만 고려해보자

카프카 클러스터에서 브로커 중 하난느 `컨트롤러` 로 '선출'된다. 토픽 파티션에는 리더와 팔로워가 있으며 메시지를 생성할 때 카프카는 레코드 파티션의 리더가 있는

브로커에게 레코드를 보낸다.

### 컨트롤러 선출

`카프카는 주키퍼를 사용해 컨트롤러 브로커를 선출한다`  컨트롤러 브로커가 실패하거나 어떤 이유로든 사용할 수 없게 되면 주키퍼는 리더의 메시지 복제를 따라잡은 것으로간주되는

`브로커 집합` 에서 새 컨트롤러를 선출한다. 이 집합을 구성하는 브로커는 동적이며 주키퍼는 이 집합의 브로커 중에서만 리더로 선출한다.



### 복제

카프카는 클러스터의 브로커가 실패할 경우 데이터 가용성을 보장하기 위해 브로커간에 레코드를 복제한다. 발행하거나 소비하는 예에서 본것과 같이 각 토픽별로 혹은 클러스터의 모든

토픽에 대해 복제수준을 설정할 수 있다. 아래그림은 브로커간 복제 흐름을 보여준다

![image-20210323152455206](/Users/kimjunseong/Library/Application Support/typora-user-images/image-20210323152455206.png)

#### `브로커 1과 3은 하나의 토픽 파티션에 대한 리더이고, 다른 하나에 대한 팔로워다. 반면 브로커2는 팔로워역할만한다 팔로어ㅜ 브로커는 리더브로커의 데이터를복사한다`

카프카의 복제 프로세스는 간단하다. 토픽 파티션을 따르는 브로커는 토픽-파티션 리더로부터 메시지를 소비하고 그 레코드를 로그에 추가한다. 

리더 브로커를 따르는 팔로워 브로커는 `ISR` 로 간주한다. 현 리더가 실패하거나 사용할 수 없게 되면 ISR 브로커는 리더로 선출될 자격이 있다.



### 컨트롤러의 책임

컨트롤러 브로커는 토픽의 모든 파티션에 대한 리더/팔로워 관계를 설정한다. 카프카노드가 죽거나 응답하지 않으면 할당된 모든 파티션이 컨트롤러 브로커에 의해 

재할당된다. 아래의 그림은 컨트롤러 브로커를 보여준다

![image](https://user-images.githubusercontent.com/40031858/112103285-e1b1ca00-8bec-11eb-960e-7742531fceea.png)

#### `컨트롤러 브로커는 일부 토픽/파티션은 리더가 되도록, 그 외에는 팔로워가 되도록 브로커에 할당하는 책임을 갖는다.`

#### `브로커를 사용할 수 없게 되면 컨트롤러 브로커는 실패한 브로커에 있던 파티션을 클러스터의 다른 브로커에 재할당한다.`

위의 그림은 간단한 실패 시나리오를 보여준다 1단계에서 컨트롤러 브로커는 브로커3을 사용할 수 없음을 감지한다. 2단계에서 컨트롤러 브로커는 브로커3의

파티션 리더십을 브로커2에 다시 할당한다

주키퍼는 카프카 운영의 다음 측면도 관여한다

- `클러스터 멤버쉽`: 클러스터에 가입하고 클러스터 멤버쉽을 유지관리한다. 브로커를 사용할 수 없게되면 주키퍼는 클러스터 멤버쉽에서 브로커를 제거한다
- `토픽설정`: 클러스터의 토픽을 트래킹한다. 브로커가 토픽의 리더인지, 토픽에 파티션이 몇 개인지, 토픽의 특정설정이 업데이트 됐는지 확인한다
- `접근 제어`: 특정 토픽에 대해 누가 읽고 쓸수있는지 식별한다

이렇듯 카프카가 팔로워들과 리더 브로커를 가질수있게 하는 것이 `주키퍼`다. 컨트롤러 브로커는 복제를 위해 토픽 파티션을 팔로워에게 할당하는 것은 물론 멤버로

있던 브로커가 실패할 때 이를 다시 저장하는 중요한 역할도 한다.

### 로그 관리

로그가 계속 커질때 어떻게 관리될까?? 클러스터에 있는 디스크의 공간은 한정된 자원이므로 카프카가 시간이 지남에 따라 메시지를 제거할 수 있어야한다. 카프카에서

오래된 데이터를 제거할 때는 두가지 방식이있는데 `로그삭제`, 그리고 `압축` 이다.

### 로그 삭제

로그 삭제 전략은 2단계 접근법이다. 먼저 로그를 세그먼트로 나누어 가장 오래된 세그먼트를 삭제한다. 로그가 증가하는 크기를 관리하기 위해 카프카는 이를 세그먼트로 분할한다

로그분할 시간은 메시지에 포함된 타임스탬프를 기반으로 한다. 카프카는 새 메시지 도착할 때의 타임스탬프가 해당 로그의 첫 번째 메시지의 타임스탬프와 `log.roll.ms`설정값을

더한 값보다 크다면 로그를 분할하고 새로운 세그먼트를 새 활성 로그로 생성된다. 이전 활성 세그먼트는 여전히 컨슈머가 메시지를 검색하는데 사용한다. 로그롤링은 카프카 브로커를

설정할 때 지정할 수 있는 구성설정이다 로그 롤링에는 두가지 옵션이 있다.

- `log.roll.ms`: 주 설정이지만 기본값은 없다
- `log.roll.hours`:보조설정이며 log.role.ms가 설정되지 않은 경우에만 사용한다 기본값은 168시간이다.

시간이 지남에 따라 세그먼트 수는 계속 증가할 것이고 오래된 세그먼트는 수신 데이터를 위한 여유공간을 확보하기 위해 삭제되어야 한다. 삭제 처리를 하기 위해 세그먼트를 보유할

기간을 지정할 수 있다. 

![image](https://user-images.githubusercontent.com/40031858/112104634-cc3d9f80-8bee-11eb-864d-f1b49d92fdb4.png)

#### `왼쪽에는 현재 로그 세그먼트가 있다. 오른쪽 상단에 삭제된 로그 세그먼트가 있으며, 그아래에 있는 세그먼트는 아직 사용중인 최근 분리된 세그먼트다`

로그롤링과 마찬가지로 세그먼트 제거도 메시지의 타임스탬프를 기반으로한다. 그저 특정시간이나 파일이 마지막으로 수정된 시간이 아니다. 로그 세그먼트 삭제는 로그의 가장

큰 타임스탬프를 사용한다. 다음처럼 우선순위에 따라 로그 세그먼트 삭제는 로그의 가장 큰 타임스탬프를 사용한다. 다음처럼, 우선 순위에 따라 세가지 설정이 있는데,

앞부분이 뒷부분 설정 항목보다 우선한다

- `log.retention.ms`: 로그 파일을 밀리초 단위로 보관하는 기간
- `log.retention.minutes`: 로그 파일을 분 단위로 보관하는 기간
- `log.retention.hours`: 시간 단위의 로그 파일 보존 기간 

주어진 기간 동안 최대 파일 크기에 도달할 것으로 확신되는 대량 토픽을 가정해 이설정을 지정한다.  `log.retention.bytes` 라는 또 다른 구성 설정은 I/O작업을 낮추기

위해 더 긴 롤링 시간 임곗값으로 지정할 수 있다. 비교적 큰 롤링 설정이 있을때 볼륨이 크게 튀는 것을 방지하기 위해 `log.segment.bytes` 설정으로 개별 로그 세그먼트의 크기를 

관리할 수 있다. 로그의 삭제는 키가 없는 레코드 또는 독립형 레코드에 대해 잘 작동한다. 그러나 키를 가진 데이터와 예상 업데이트가 있다면 더 잘맞는 다른 방법이있다

### 로그 압축

키를 가진 데이터가 있고 시간이 지나면서 해당 데이터에 대한 업데이트를 받는 경우를 생각해보자. 동일한 키를 가진 새레코드가 이전값을 업데이트한다는 것을 의미한다. 예를 들어

주식 종목 코드가 키가되고, 주당 가격은 정기적으로 업데이트되는 값이 될 수 있다. 이 정보를 사용해 주식값을 보여주는데 크래시가 발생하거나 다시시작 됐다고 생각해보자 

각 키에 대한 최신 데이터로 백업을 시작할 수 있어야한다. 

삭제 정책을 사용하면 마지막 업데이트와 애플리케이션의 크래시나 재시작 사이에 세 그먼트가 제거됐을 수 있다. 시작할때 모든 레코드가 있지않을수 있다. 데이터베이스 테이블에

업데이트하려는 것과 동일한 키를 이용해 다음 레코들르 처리하고 그 키에 대한 마지막 값을 유지하는 방법이 더 나을것이다. 키로 레코드를 업데이트하는 것은 압축된 토픽이 제공하

는 동작이다. 압축을하면 대략적인 접근 방식을 사용하고 로그에서 키별로 오래된 레코드를 삭제한다. 아주 높은 레벨에서 로그 클리너는 백그라운드로 실행되어 로그 세그먼트 파일을

다시 복사하고, 같은 키를 가진 것이 그 이후의 로그에 생긴다면 기존 레코드는 제거한다. 아래의 그림이 로그압축이 각 키별로 가장 최근의 메시지를 유지하는 방법이다.

![image](https://user-images.githubusercontent.com/40031858/112106985-bf6e7b00-8bf1-11eb-8392-991ae200cef4.png)

#### `압축 전의 로그가 왼쪽에 있다. 중복된 키가 해당 키의 업데이트된 다른 값을 갖고 있음을 알 수 있다. 오른쪽은 압축 후의 로그다. 각키의 최신값은 유지되지만 로그의 크기는 더작다`

이 방법은 주어진 키에 대한 마지막 레코드가 로그에 있음을 보장한다. 토픽별로 로그 보존 방법을 지정할 수 있으므로 어떤 토픽은 시간 기반 보존을 하고 다른 토픽은 압축을

사용하는 것이 가능하다. 기본적으로 로그 클리너는 활성화되어 있다. 토픽에 대한 압축을 사용하려면 토픽을 생성할때 `log.cleanup.policy=compact` 속성을 설정하면 된다

```markdown
`참고` cleanup.policy가 compact일때 어떻게 로그에서 레코드를 제거할 수 있는지 궁금할 수 있다. 압축된 토픽에서 삭제는 주어진 키에 null값을 제공하고
삭제표시를 설정한다. null값을 가진 키는 동일한 키를 가진 이전 레코드가 제거됐음을 보장하며, 일정 시간 후에는 삭제 표시된 레코드 자체가 제거된다
```

독립적인 이벤트나 메시지가 있다면 로그삭제를 사용하자. 이벤트나 메시지에 대한 업데이트가 있는경우 로그압축을 사용하자!

## 🍰 프로듀서로 메시지 보내기

중앙 집중식 판매 거래 데이터 허브에 대한 gs25요구로 돌아가서, 구매 거래를 카프카로 보내는 방법을 살펴보자. 카프카에서 `프로듀서`는 메시지를 보내는데 사용하는 클라이언트다.

![image](https://user-images.githubusercontent.com/40031858/112108281-54be3f00-8bf3-11eb-9443-2c9fd3c49eba.png)



#### `프로듀서는 카프카에 메시지를 보내는데 사용한다. 프로듀서는 어떤 컨슈머가 언제 메시지를 읽을지 모른다`

gs25에 판매 거래가 많긴 하지만 10000원 책과 같은 단품구매를 고려하자 고객이 판매거래를 완료하면 정보가 키/값 쌍으로 변환되어 프로듀서를 통해 카프카로 전송된다

키는 고객 ID 12347777이고 값은 "{\"item\" : \"book\", \"price\" : 10000}"와 같은 JSON형식이다. 이런 형식의 데이터로 프로듀서가 데이터를 카프카 클러스터로 보낼 수 있다.

```java
Properties properties=new Properties();
properties.put("bootstrap.servers","localhost:9092");

properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
properties.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");
properties.put("acks","1");
properties.put("retries","3");
properties.put("compression.type","snappy");
properties.put("partitioner.class",PurchaseKeyPartitioner.class.getName()); //프로듀서를 설정하기 위한 속성

PurchaseKey key=new PuchaseKey("1234568",new Date());
try(Producer<PuchaseKey,String> producer= new KafkaProducer<>(properties)){
  ProducerRecord<PurchaseKey,String> record= new ProducerRecord<>("transactions",key, "{\"item\" : \"book\", \"price\" : 10000}");
  
  Callback callback=(metadata, exception) -> {
    if(exception!=null){
      System.out.println("Encountered exception" + exception);
    }
  };
  Future<RecordMetadata> sendFuture= producer.send(record,callback);
}
```





카프카 프로듀서는 `스레드세이프` 하다. 카프카로의 모든 전송은 비동기식이다. `Producer.send` 는 프로듀서가 내부 버퍼에 레코드를 저장하면 즉시 반환한다. 버퍼는 배치로 레코드를 보낸다.

설정에 따라 프로듀서의 버퍼가 가득 차있는 동안 메시지를 보내려고 하면 블로킹이 있을 수 있다. 여기 설명한 `Producer.send` 메소드는 `Callback` 인스턴스를 사용한다. 

리더 브로커가 레코드를 승인하면 프로듀서는 `Callback.onComplete` 메소드를 부른다. `Callaback.onComplete` 메소드에서 인수 중 하나만 널이 아니다. 이 경우 오류 이벤트의

스택 트레이스 출력에만 관심이 있어서 예외 객체가 널이 아닌지 확인한다. 서버가 레코드를 확인하고 나면 반환된 `Future`는 `RecordMetadata` 객체를 생성한다.



### 프로듀서 속성

`KafkaProducer` 인스턴스를 만들 때 프로듀서이ㅡ 설정을 포함하는 `java.util.Properties` 매개변수를 전달했다 `KafkaProducer` 의 설정은 복잡하지 않지만 설정할 때

고려해야 할 핵심 속성이 있다. 이설정은 예를 들면 사용자 정의 파티셔너를 지정하는 것이다. 속성이 너무많으니 일부만 보면 당므과 같다

- `부트스트랩 서버`:  bootstrap.servers는 쉼표로 구분된 호스트:포트값의 리스트다. 프로듀서는 클러스터의 모든 브로커를 사용하며, 이리스트는 처음에 클러스터에 연결하는 용도로만 사용
- `직렬화` : key.serializer와 value.serializer는 키와 값을 바이트 배열로 변환하는 방법을 카프카에 알려준다. 내부적으로 카프카는 키와 값에 바이트 배열을 사용하기 때문에 카프카에 정확한
- 직렬화기 를 제공해 전송하기 전에 객체를 바이트 배열로 변환해야한다
- `acks`: acks는 레코드 전송이 완료됐다고 생각하기 전에 프로듀서가 브로커로부터 기다리는 확인 수를 지정한다. acks에 유효한 값은 all,0,1이다. all값을 사용하면 브로커는 모든 
- 팔로워가 레코드를 커밋할 때까지 대기한다. 1로 설정하면 브로커는 레코드를 로그에 기록하지만 팔로워의 레코드 커밋에 대한 확인 응답을 기다리지 않는다 0값은 프로듀서가
- 어떤 확인 응답도 기다리지 않음을 의미한다. 일반적으로 보내고 잊어버린다
- `재시도` : 배치결과가 실패하는 경우 retries는 재전송 시도 횟수를 지정한다. 레코드 순서가 중요한 경우 max.in.flight.request.per.connection을 1로 설정해 실패한 레코드가
- 재전송되기 전에 두 번째 배치가 성공적으로 보내지는 시나리오를 방지해야한다
- `압축 타입`: compression.type은 적용할 압축 알고리즘이 있으면 지정한다. 설정하면 compression.type은 보내기 전에 배치를 압축하도록 프로듀서에 지시한다. 개별레코드가아닌 배치단위로 압축
- `파티셔너 클래스` : partitioner.class는 Partitioner인터페이스를 구현하는 클래스의 이름을 지정한다. partitioner.class는 사용자정의 파티서녀에 대한 설명과 관련있다.!

### 파티션과 타임스탬프 지정

`ProducerRecord`를 생성할 때 파티션, 타임스탬프 또는 둘 다 지정할 수 있다. 위에서 ProducerRecord를 생성할때 네가지 오버로드된 생성자 중 하나를 사용했다. 다른생성자

는 파티션과 타임스탬프 또는 파티션만 설정할 수 있다

```java
ProducerRecord(String topic, Integer partition, String key, String value)
ProducerRecord(String topic, Integer partition, Long timestamp, String key, String value)
```

### 파티션 지정

파티션을 명시적으로 설정하는 경우는 어떤 경우일까? 키를 가진 입력이 유입되고 있다고 가정하되, 사용자는 키에 있는 모든값을 처리하는 로직을 갖고 있기 때문에 어떤 파티션으로

레코드가 갈지는 중요하지 않다고 하자. 덧붙여 키 분배가 고르지 않을 수 있고 모든 파티션이 대략 같은 양의 데이터를 받길 원한다. 다음은 이를 수행하는 대략적 구현이다

```java
AtomicInteger partitionIndex=new AtomicInteger(0); 
int currentPartition=Math.abs(partitionIndex.getAndIncrement())%numberPartitions;
ProducerRecord<String, String> record= new ProducerRecord<>("topic",currentPartition,"key","value");
```



### 카프카의 타임 스탬프 

카프카 0.10버전부터 레코드에 타임스탬프를 추가했다. 다음의 오버로드된 생성자 호출을 통해 `ProducerRecord` 를 만들 때 타임스탬프를 설정한다

​	ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value)

타임 스탬프를 설정하지 않으면 프로듀서가 레코드를 카프카 브로커에 보내기 전에 현재시간을 설정할 것이다. 타임스탬프는 `log.message.timestamp.type` 브로커 설정의 영향도받는다

이 설정은 CreateTime(기본값) 또는 LogAppendTime으로 설정할 수 있다. 다른 브로커 설정과 마찬가지로 브로커에 구성된 값은 모든 토픽에 기본값으로 적용하지만, 토픽을 만들

때 해당 토픽에 다른 값을 지정할 수 있다. 브로커에 LogAppendTime을 지정하고 토픽이 브로커의 설정을 덮어쓰지 않으면 브로커는 로그에 레코드를 추가할 때 현재 시간으로 

타임스탬프롤 덮어쓴다. 그렇지 않으면 ProducerRecord의 타임스탬프가 사용된다.

왜 그중 꼭 하나를 선택할까?? LogAppendTime은 '처리시간'으로 간주되며, CreateTime은 '이벤트 시간'으로 간주한다. 어떤 것을 사용할지는 비즈니스 요구사항에 따라 다르다.

## 💚 컨슈머로 메시지 읽기

최신 gs25 판매 통계를 보여주는 프로토타입 애플리케이션을 작성한다고 해보자. `KafkaConsumer`는 카프카 메시지를 소비하는데 사용할 클라이언트다

`KafkaConsumer` 클래스는 사용하기 쉽지만 운영상 고려해야할 사항이 있다. 아래 그림은 컨슈머가 데이터 흐름에서 어떤 역할을 하는지 강조하는 아키텍처를 보여준다

![image](https://user-images.githubusercontent.com/40031858/112111981-46265680-8bf8-11eb-8b76-056011fb8fe0.png)

#### `카프카로부터 메시지를 읽는 컨슈머들이다. 프로듀서가 컨슈머에 대해 알지 못하는 것처럼 컨슈머는 누가 메시지를 만들었는지 모르고 카프카로부터 메시지를 읽는다`

### 오프셋 관리

`KafkaProducer` 는 본질적으로 상태가 없지만 `KafkaProducer`는 주기적으로 카프카에서 소비되는 메시지의 오프셋을 커밋해 일부 상태를 관리한다. 오프셋은 메시지를 고유하게

식별하고 로그에서 메시지의 시작위치를 나타낸다. 컨슈머는 받은 메시지의 오프셋을 주기적으로 커밋해야한다. 오프셋 커밋은 컨슈머에 있어서 두가지 의미가 있다

- 커밋한다는 것은 컨슈머가 메시지를 완전히 처리했음을 의미한다
- 커밋은 실패나 재시작시 해당 컨슈머의 시작지점도 나타난다

새로운 컨슈머 인스턴스가 있거나 일부 오류가 발생했고 마지막으로 커밋한 오프셋을 사용할 수없는 경우 컨슈머가 시작하는 위치는 설정에 따라 다르다

- `auto.offset.reset="earliest"`: 사용 가능한 가장 이른 오프셋부터 시작해 메시지를 가져올 것이다. 로그 관리 프로세스에 의해 아직 제거되지않은 메시지가 검색된다
- `auto.offset.reset="latest"`: 가장 최신 오프셋에서 메시지를 읽어서 기본적으로 컨슈머가 클러스터에 합류한 지점부터 유입된 메시지만 소비한다
- `auto.offset.reset="none"`: 재설정 전략을 지정하지 않았다. 브로커가 컨슈머에게 예외를 발생시킨다

아래 그림에서 auto.offset.reset설정을 선택했을 때의 영향을 확인할 수 있다. earliest를 선택하면 오프셋 1에서 시작하는 메시지를 받고 latest를 선택하면 11에서시작하는 메시지를읽는다.

![image](https://user-images.githubusercontent.com/40031858/112112835-5ee33c00-8bf9-11eb-860d-aed6a9ab52d4.png)

다음으로 오프셋 커밋에 관한 옵션을 살펴보자. 자동이나 수동으로 이 작업을 수행할 수 있다.

### 자동 오프셋 커밋

자동 오프셋 커밋 방식이 기본값이며. `enable.auto.commit` 프로퍼티로 설정할 수 있다. 짝을 이루는 설정옵션은 `auto.commit.interval.ms`인데, 컨슈머가 오프셋을 커밋하는

주기를 지정한다(기본값은 5초) 이값을 조정할때 주의해야하는데 너무 작으면 네트워크 트래픽을 증가시키고, 너무 크면 실패 시 재시작 이벤트에서 컨슈머가 이미 받았던 데이터를 다시받게될 수 있다.

### 수동 오프셋 커밋

수동 커밋된 오프셋에는 동기식 및 비동기식의 두 가지 유형이 있다. 동기화 머킷은 다음과 같다

```java
consumer.commitSync()
consumer.commitSync(Map<TopicPartition, OffsetAndMetadata>)
```



인수가 없는 `commitSync()` 메소드는 마지막 검색(폴링) 에서 반환된 모든 오프셋이 성공할 때까지 블로킹한다. 이 호출은 구독한 모든 토픽과 파티션에 적용된다. 다른 버전은

`Map<TopicPartition, OffsetMeatadata>` 매개변수를 취하고 맵에 지정된 오프셋, 파티션, 토픽만 커밋한다.

유사한 `consumer.commitAsync()`메소드가 있으며 완전 비동기식이고 즉시 반환된다. 오버로드된 메소드 중 하나는 인수를 허용하지 않고 두 `consumer.commitAsync` 메소드는

커밋이 성공적으로 또는 오류로 종료될 때 호출되는 `OffsetCommitCallback` 객체를 제공하는 옵션을 갖는다. 콜백 인스턴스를 제공하면 비동기 처리 및 오류처리가 가능하다.

수동 커밋을 사용하면 레코드가 처리된 것으로 간주되는 시기를 직접 제어할 수 있다는 장점이 있다.

### 컨슈머 생성하기

자바의 `java.util.Properties` 객체 형태로 설정을 제공하면 `KafkaConsumer` 인스턴스를 얻을 수 있다. 그런 다음 이 인스턴스는 제공된 토픽 이름 목록이나 정규 표현식으로

지정된 토픽을 구독한다. 일반적으로 루프에서 컨슈머를 실행해 밀리초 단위로 지정된 기간동안 폴링한다.

`consumerRecords<K,V>` 객체는 폴링의 결과다. `ConsumerRecords`는 `Iterable` 인터페이스를 구현하고 `next()`를 호출할 때마다 실제 키와 값 이외의 메시지에 대한

메타데이터가 포함된 `ConsumerRecord`객체를 반환한다. 마지막 `poll`호출에서 반환된 `ConsumerRecord` 객체를 모두 사용한 후에는 루프의 맨위로 돌아가 지정된 기간 동안

다시 폴링한다. 실제로 오류가 발생하거나 애플리케이션을 종료하고 다시 시작해야하는 경우가 아니라면 컨슈머는 이러한 방식으로 무기한 실행해야 한다.

### 컨슈머와 파티션

일반적으로 토픽의 각 파티션마다 하나씩 여러 컨슈머 인스턴스가 필요할 것이다. 한 컨슈머가 여러 파티션에서 읽도록 할 수 있지만, 파티션 수만큼 스레드 풀에 스레드가 있는 컨슈머가

하나의 파티션에 배정되는 것이 일반적이지는 않다. 이 파티션당 컨슈머 패턴은 처리량을 최대화하지만, 여러 애플리케이션이나 머신에 컨슈머를 분산하는 경우 모든 인스턴스의 총

스레드 수는 해당 토픽의 총 파티션 수를 넘기지 않아야한다. 전체 파티션수를 초과하는 스레드는 유휴 상태가 되기 때문이다. 컨슈머가 실패하면 리더브로커는 파티션을 다른 활성

컨슈머에게 할당한다. 리더 브로커는 동일한 `group.id` 를 가진 사용 가능한 모든 컨슈머에게 토픽 파티션을 할당한다. `group.id`는 컨슈머를 `컨슈머 그룹`에 속하도록

식별하는 설정이다. 이렇게 하면 컨슈머는 동일한 머신에 있을 필요없다. 실제로 컨슈머를 몇대의 머신에 분산하는 것이 더 좋다. 그렇게 하면 하나의 머신이 실패할 경우, 리더 브로커는

상태가  좋은 머신의 컨슈머에게 토픽 파티션을 할당할 수 있다.

### 리밸런싱

컨슈머에게 토픽-파티션 할당을 추가 및 제거하는 프로세스를  `리밸런싱` 이라고한다. 컨슈머에 대한 토픽-파티션 할당은 정적이지 않고 동적이다. 동일한 그룹 ID를 가진 컨슈머를 추가하면

현재 토픽-파티션 할당 중 일부를 활성화 상태의 기존 컨슈머에게 가져와 새로운 컨슈머에게 준다. 이 재할당 프로세스는 모든 파티션이 데이터를 읽는 컨슈머에게 할당될때까지 계속된다

균형이 맞은다음 추가 컨슈머는 유휴상태로 남을것이다 컨슈머가 어떤이유로든 그룹을 떠난다면 토픽-파티션 할당이 다른 컨슈머에게 재할당된다 

### 더 세분화된 컨슈머 할당

카프카는 토픽-파티션 부하 균형을 모든 컨슈머에 걸쳐 조정하지만 토픽과 파티션 할당은 미리 결정되지 않는다. 즉 , 각 컨슈머가 받을 토픽-파티션 쌍을 알수 없다.

`KafkaConsumer`에는 특정 토픽과 파티션을 구독할 수 있는 메소드도 있다

```java
TopicPartition fooTopicPartition_0=new TopicPartition("foo",0);
TopicPartition barTopicPartition_0=new Topicpartition("bar",0);

consumer.assign(Arrays.asList(fooTopicPartition_0,barTopicPartition_0));
```

이처럼 수동으로 토픽 파티션을 할당할 때 다음처럼 고려해야 할 트레이드 오프가 있다.

- 동일한 그룹 ID를 가진 컨슈머라 하더라도 오류가 발생하면 토픽 파티션이 재할당되지않는다. 할당을 변경하려면 consumer.assign을 다시호출해야한다
- 컨슈머에 지정된 그룹이 커밋에 사용되긴 하지만, 각 컨슈머가 독립적으로 작동하기 때문에 각 컨슈머에게 고유한 그룹 ID를 부여하는 것이 좋다

### 컨슈머 예제

```java
public void startConsuming(){
  executorService=Executors.newFixedThreadPool(numberPartitions);
  Properties properties=getConsumerProps();
  
  for(int i=0; i<numberPartitions; i++){
    Runnable consumerThread=getConsumerThread(properties); //컨슈머 스레드 생성
    executorService.submit(consumerThread);
  }
}

private Runnable getConsumerThread(Properties properties){
  return() -> {
    Consumer<String, String> consumer=null;
    try{
      consumer=new KafkaConsumer<>(properties);
      consumer.subscribe(Collections.singletonList("test-topic"));
      while(!doneConsuming){
        ConsumerRecords<String, String> records=consumer.poll(5000);
        for(ConsumerRecord<String,String> record: records){
          String message=String.format("Consumed: key= %s value = %s with offset = %d partition = %d",
                                      record.key(), record.value(), record.offset(), record.partition());
          System.out.println(message);
        }
      }
    }catch(Exception e){
      e.printStackTrace();
    }finally{
      if(consumer!=null){
        consumer.close(); //컨슈머 닫기 (안닫으면 자원누수가 생김)
      }
    }
  }
}
```

# 2장 요약

#### [1] 카프카는 메시지를 수신해 컨슈머의 요청에 쉽고 빠르게 응답할 수 있는 방식으로 메시지를 저장하는 메시지 브로커다. 메시지는 컨슈머로 전송후에도 사라지지 않으며, 

#### 카프카의 메시지 보존은 메시지를 소비하는 시기와 빈도에 전적으로 의존한다.

#### [2] 카프카는 높은 처리량을 얻기위해 파티션을 사용하고 같은 키를 사용해 메시지를 순서대로 그룹화하는 방법을 제공한다

#### [3] 프로듀서는 카프카에 메시지를 보내는데 사용한다

#### [4] 널 키는 라운드 로빈 파티션 할당을 의미한다. 그렇지 않으면 프로듀서는 파티션 할당을 위해 키의 해시와 파티션수의 모듈러 값을 사용한다

#### [5] 컨슈머는 카프카에서 온 메시지를 읽는데 사용한다

#### [6] 컨슈머 그룹의 일원인 컨슈머는 메시지를 고르게 분산하기 위해 토픽-파티션 할당을 받는다 



